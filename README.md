# Reinforced Frog

Jumper Frog in python with AI reinforcement üê∏

## Status

| Branch | Quality Gate                                                                                                                                                                                                                                                                                              | Bugs                                                                                                                                                                                                                                                                               | Vulnerabilities                                                                                                                                                                                                                                                                                          | Code Smells                                                                                                                                                                                                                                                                                      | Reliability                                                                                                                                                                                                                                                                                                    | Security                                                                                                                                                                                                                                                                                                 | Maintainability                                                                                                                                                                                                                                                                                              | Technical Debt                                                                                                                                                                                                                                                                                      |
|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Main   | [![Quality Gate Status](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=alert_status&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Bugs](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=bugs&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Vulnerabilities](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=vulnerabilities&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Code Smells](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=code_smells&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Reliability Rating](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=reliability_rating&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Security Rating](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=security_rating&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Maintainability Rating](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=sqale_rating&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Technical Debt](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=sqale_index&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) |
| Dev    | [![Quality Gate Status](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=alert_status&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Bugs](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=bugs&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Vulnerabilities](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=vulnerabilities&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Code Smells](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=code_smells&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Reliability Rating](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=reliability_rating&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Security Rating](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=security_rating&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Maintainability Rating](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=sqale_rating&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Technical Debt](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=sqale_index&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         |

|                                                             |                                            |
|-------------------------------------------------------------|--------------------------------------------|
| ![README-1666710399073.gif](./doc/README-1666710399073.gif) | <img width="250px" src="armored_frog.png"> |

## Pr√©sentation du jeu et Objectif

L'objectif principal est de faire apprendre par renforcement un agent sur le jeu Frogger.

### Contexte

Ce projet a √©t√© r√©alis√© dans le cadre du cours d'apprentissage par renforcement. Il a √©t√© r√©alis√© par 3 √©tudiants en
5·µâ ann√©e d'architecture logicielle.

### Frogger original

| R√®gles du jeu                                                                                                                                                                                                                                                                                                                                                                         | Jeu original                                           |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------|
| Frogger est un jeu d'arcade classique. Le but du jeu est de diriger des grenouilles jusqu'√† leurs maisons. Pour cela, le joueur doit d'abord traverser une route en √©vitant des voitures qui roulent √† diff√©rentes vitesses puis une rivi√®re aux courants changeants et enfin, √† nouveaux une route. La grenouille meurt si elle touche une voiture ou si elle tombe dans la rivi√®re. | <img width="1500px" src="doc/Frogger_game_arcade.png"> |

### Objectif

L'objectif est de faire apprendre √† un agent √† traverser la route et la rivi√®re en √©vitant les voitures et l'eau.
Pour cela, nous allons utiliser l'algorithme `Q-Learning`. L'agent va apprendre √† traverser la route et la rivi√®re en
apprenant √† associer une action √† un √©tat. L'agent va donc d√©couvrir comment associer une action √† un √©tat.

Pour cela, nous allons √©galement devoir d√©velopper le jeu Frogger en utilisant la
librairie [`arcade`](https://api.arcade.academy/en/latest/). Le seul langage
utilis√© est le Python, nous n'utilisons pas de librairie externe mis √† part arcade et quelques librairies utilitaires.

## Installation

### Pr√©requis

- Python 3.10 minimum
- PIP3

### Installation des d√©pendances

Apr√®s avoir clon√© le projet, il faut installer les d√©pendances avec la commande suivante :

```bash
pip3 install -r requirements.txt
```

## Utilisation

### Environnement

Avant de lancer le jeu, il faut cr√©er le fichier `.env` √† la racine du projet. Ce fichier contient les variables
d'environnement n√©cessaires au bon fonctionnement du jeu. Vous pouvez vous baser sur le
fichier [`.env.example`](./.env.example) pour cr√©er le fichier `.env`.

| Variable                   | Description                                                                                       | Valeur conseill√©e                          |
|----------------------------|---------------------------------------------------------------------------------------------------|--------------------------------------------|
| AGENT_COUNT                | Nombre d'agent en simultan√© sur la carte                                                          | 1-10                                       |
| AGENT_DEBUG                | Afficher les informations debugs en console des agents (WIN/LOOSE)                                | `false`                                    |
| ARCADE_INSIGHTS            | Afficher les informations de l'agent sur le jeu `arcade`                                          | `true`                                     |
| AGENT_GAMMA                | Taux de prise en compte de l'√©tat futur                                                           | 0.1                                        |
| AGENT_LEARNING_FILE        | Emplacement du fichier qtable                                                                     | qtable/**<nom-du-fichier>**.xz             |
| AGENT_LEARNING_RATE        | Taux d'apprentissage de l'agent                                                                   | 0.6                                        |
| AGENT_VISIBLE_COLS_ARROUND | Nombre de colonnes visible autour de l'agent dans son environnement                               | 4-6                                        |
| AGENT_VISIBLE_LINES_ABOVE  | Nombre de lignes visible devant l'agent dans son environnement                                    | 1-2                                        |
| EXPLORE_RATE               | Taux d'exploration sur les actions d√©termin√©s de l'agent                                          | 0.05-0.1                                   |
| EXPLORE_RATE_DECAY         | Taux de diminution du taux d'exploration                                                          | 0.999                                      |
| GENERATE_HISTORY_GRAPH     | G√©n√©rer le graphique de progression d'apprentissage en m√™me temps que la sauvegarde de la Q-Table | `true`                                     |
| HASH_QTABLE                | Hash des lignes de l'environnement (permet de diminuer un peu la taille en m√©moire)               | `false`                                    |
| LEARNING_MODE              | Passer en mode apprentissage (console seulement) ou en mode graphique (arcade)                    | `true` pour apprendre un peu, puis `false` |
| LEARNING_TYPE              | Type d'apprentissage (**QLEARNING**, **MQLEARNING**, **DQLEARNING**)                              | **QLEARNING**-**MQLEARNING**               |
| LEARNING_TIME              | Temps de l'apprentissage en minute                                                                | 45                                         |
| LEARNING_PRINT_STATS_EVERY | Afficher en console les stats d'appretissage toute les x secondes                                 | 30-60                                      |
| LEARNING_SAVE_QTABLE_EVERY | Fr√©quence de sauvegarde de la Q-Table toute les x secondes (op√©ration lourde)                     | 60-600                                     |
| QTABLE_HISTORY_FILE        | Emplacement des fichiers d'historique                                                             | history/**<nom-du-fichier>**.history       |
| QTABLE_HISTORY_PACKETS     | Paquets pour l'historique                                                                         | = au nombre d'agents (1-10)                |
| WORLD_TYPE                 | Type de monde ( 0 -> Route + Eau, 1 -> Route seulement, 2 -> Eau seulement)                       | 0                                          |

Pour lancer le jeu, il faut lancer la commande suivante :

```bash
python3 main.py
```

## D√©veloppement du jeu

### Pr√©sentation de la librairie arcade

[`Arcade`](https://api.arcade.academy/en/latest/) est une librairie Python permettant de cr√©er des jeux vid√©o. Elle est
bas√©e sur Pyglet et permet de cr√©er des jeux vid√©o 2D. Elle permet de cr√©er des jeux vid√©o en 2D avec des sprites, des
animations, des sons, des effets de particules, ...

![img.png](doc/img.png)

### Configuration des r√®gles

Afin de pouvoir modifier rapidement la configuration de notre jeu (difficult√©, tokens, actions possibles, ...), nous
avons
√©crit toute la configuration dans le fichier [`config.py`](./conf/config.py). Ce fichier est lu par le jeu.

#### Tokens

Ce fichier de configuration contient les diff√©rents tokens utilis√©s dans le jeu. Ces derniers permettent au jeu d'avoir
une repr√©sentation textuelle de son environement, ce qui va grandement nous aider pour l'apprentissage de l'agent.

```python
CAR_TOKEN = 'C'
TRUCK_TOKEN = 'Z'
TURTLE_TOKEN = 'T'
TURTLE_L_TOKEN = 'TL'
TURTLE_XL_TOKEN = 'TXL'
REVERSED_CAR_TOKEN = 'RC'
REVERSED_TRUCK_TOKEN = 'RZ'
REVERSED_TURTLE_TOKEN = 'RT'
REVERSED_TURTLE_L_TOKEN = 'RTL'
...

ACTION_UP = 'U'
ACTION_DOWN = 'D'
ACTION_LEFT = 'L'
ACTION_RIGHT = 'R'
ACTION_NONE = 'N'
...

WATER_COMMONS_TOKENS = [TURTLE_TOKEN, TURTLE_L_TOKEN, TURTLE_XL_TOKEN, REVERSED_TURTLE_TOKEN, REVERSED_TURTLE_L_TOKEN,
                        ...]
...

WIN_STATES = [EXIT_TOKEN]

```

#### Arcade

Ce fichier de configuration contient les diff√©rents param√®tres de la librairie arcade. Ces derniers permettent de
d√©finir les sprites des diff√©rentes entit√©s, ainsi que leur taille et le scaling.

```python
SCALE = 1
SPRITE_SIZE = 64 * SCALE

...


def get_sprite_resources(name: str, sprite_size: float = 0.5):
  return arcade.Sprite(f":resources:images/{name}.png", sprite_size * SCALE)


def get_sprite_local(name: str, sprite_size: float = 0.5):
  return arcade.Sprite(f"assets/sprite/{name}.png", sprite_size * SCALE)


ENTITIES: Dict[str, WorldEntity] = {
  CAR_TOKEN: WorldEntity(1, 1, CAR_TOKEN, get_sprite_local("car_1", 0.65)),
  ...
}

...

WORLD_WIDTH = 180
WORLD_HEIGHT = 117
WORLD_SCALING = 9
```

### Repr√©sentation du monde

Le monde est repr√©sent√© par une matrice de caract√®res. Chaque caract√®re repr√©sente une entit√© du monde. Les entit√©s
sont repr√©sent√©es par des tokens. Ces derniers sont d√©finis dans le fichier de configuration.

La classe permettant de repr√©senter le monde est la classe [`World`](./game/world.py). Cette classe permet de

Cette derni√®re permet de :

- Cr√©er un monde, avec la bonne configuration
- G√©rer la mise √† jour (d√©placement) des entit√©s dans le monde
- G√©rer les collisions entre les entit√©s
- G√©rer les mouvements, r√©compenses des joueurs

#### Repr√©sentation du monde pour l'agent

A chaque √©tat, l'agent re√ßoit une repr√©sentation du monde sous forme de liste de cha√Æne caract√®res. Chaque √©l√©ment
repr√©sente une ligne visible ( d√©finit dans les variables d'environnement) du monde.

Ainsi, l'agent ne voit pas toute la carte, mais tout au plus 2 ligne devant lui, 1 ligne derri√®re lui, et 4 colonnes sur
les c√¥t√©s.

![Untitled Diagram.png](./doc/README-1667658404493.png)

#### World Entity

Pour g√©n√©raliser les diff√©rentes entit√©s que nous traitons, nous avons la
classe [`WorldEntity`](./display/entity/world_entity.py).
Cette derni√®re permet de regrouper pour chaques entit√©s :

- La taille de l'entit√©
- Le token de l'entit√©
- Le sprite de l'entit√©

#### World Line

La classe [`WorldLine`](./display/entity/world_line.py) permet de repr√©senter une ligne du monde. Cette derni√®re permet
de g√©rer les d√©placements des entit√©s sur chaque ligne, ainsi que leur fr√©quence d'apparition, vitesse, ...

### Joueurs

Le joueur est repr√©sent√© par l'interface [`Player`](./game/Player.py). Cette derni√®re permet de g√©rer le d√©placement du
joueur, de le g√©rer dans la classe principale `Game`. Cette interface nous permet de g√©rer plusieurs types de joueurs (
Humain, Agent).

```python
from typing import Tuple, List

from arcade import Sprite

from display.entity.world_entity import WorldEntity
from game.world import World


class Player:
  def init(self, world: World, intial_state: Tuple[int, int], _initial_environment: bytes):
    pass

  def best_move(self, environment: [str]) -> str:
    pass

  def step(self, action: str, reward: float, new_state: Tuple[int, int], _environment: List[str]):
    pass

  def save_score(self):
    pass

  def update_state(self, new_state, new_environment):
    pass

  @property
  def sprite(self) -> Sprite:
    pass

  @property
  def world_entity(self) -> WorldEntity:
    pass

  @property
  def is_human(self) -> bool:
    pass

  @property
  def score(self) -> int:
    pass

  @property
  def state(self) -> Tuple[int, int]:
    pass
```

#### Joueur Humain

Le joueur humain est repr√©sent√© par la classe [`HumanPlayer`](./game/HumanPlayer.py). Cette derni√®re permet de se
d√©placer avec les touches directionnelles du clavier.

### Affichage graphique

L'affichage graphique est enti√®rement g√©r√© par la classe [`WorldWindow`](./display/world_window.py). Cette derni√®re
permet de g√©rer l'affichage du monde sur arcade. Elle permet √©galement de g√©rer la vitesse de jeu, le nombre de frames
par seconde, les statistiques affich√©s, ...

```python
import arcade.color

from ai.Model import Model
from conf.config import *
from game.game import Game


class WorldWindow(arcade.Window):
  def __init__(self, game: Game, env, model: Model):
    super().__init__(
      int(game.world.width / WORLD_SCALING * SPRITE_SIZE),
      int(game.world.height / WORLD_SCALING * SPRITE_SIZE),
      'REINFORCED FROG',
      update_rate=1 / 60
    )
    self.__height = int(game.world.height / WORLD_SCALING * SPRITE_SIZE)

  # ...

  # ...

  def setup(self):
    self.setup_world_states()
    self.setup_players_states()
    self.setup_world_entities_state()

  def setup_world_entities_state(self):
    self.__entities_sprites = arcade.SpriteList()
    for state in self.__game.world.world_entities_states.keys():
      world_entity: WorldEntity = self.__game.world.get_world_entity(state)
      if world_entity is not None:
        sprite = self.__get_entity_sprite(state, world_entity)
        self.__entities_sprites.append(sprite)

  def setup_world_states(self):
    self.__world_sprites = arcade.SpriteList()
    for state in self.__game.world.world_states:
      world_entity: WorldEntity = self.__game.world.get_world_line_entity(state)
      if world_entity is not None:
        sprite = self.__get_environment_sprite(state, world_entity)
        self.__world_sprites.append(sprite)

  def setup_players_states(self):
    self.__players_sprites = arcade.SpriteList()
    for player in self.__game.players:
      sprite = player.sprite
      sprite.center_x, sprite.center_y = (
        self.__get_xy_state((player.state[0] + WORLD_SCALING // 2, player.state[1] + WORLD_SCALING // 2)))
      self.__players_sprites.append(sprite)

  def __draw_debug(self):

  # ...

  def on_draw(self):
    arcade.start_render()
    self.__world_sprites.draw()
    self.__entities_sprites.draw()
    self.__players_sprites.draw()
    if self.__debug == 1:
      self.__draw_debug()
    elif self.__debug == 2:
      self.__draw_collisions_debug()
    if self.__env['ARCADE_INSIGHTS']:
      self.__draw_model_insights()

  def __draw_model_insights(self):

  # ...

  def on_update(self, delta_time: float):
    self.__game.step()
    self.setup_players_states()
    self.__players_sprites.update()
    self.setup_world_entities_state()
    self.__entities_sprites.update()

  # ...
```

## D√©veloppement de l'IA

Pour l'IA, nous avons utilis√© 3 m√©thodes principales :

- Q-Learning
- Multi-Q-Learning
- Deep Q-Learning

### Q-Learning

Le Q-Learning est une m√©thode d'apprentissage par renforcement. Cette m√©thode permet de d√©terminer la meilleure action
√† effectuer dans un √©tat donn√©. Pour cela, elle utilise une fonction de valeur Q qui permet de d√©terminer la valeur
d'une action dans un √©tat donn√©. Cette fonction est mise √† jour √† chaque √©tape de l'apprentissage.

#### Impl√©mentation

L'impl√©mentation du Q-Learning est g√©r√© par la classe [`QLearning`](./ai/Qtable.py). Cette derni√®re permet de
g√©rer la table de Q-Learning, de mettre √† jour les valeurs de la table, de r√©cup√©rer la meilleure action √† effectuer,
de sauvegarder la table de Q-Learning, ...

Cette classe permet √©galement (comme les autres m√©thodes d'apprentissage) de g√©rer l'exploration et l'exploitation,
ainsi que l'historique de progression.

Nous nous basons sur l'√©quation de Bellman pour mettre √† jour la table de Q-Learning :

![0 EezURPNjW2U6EYpA.jpeg](./doc/README-1667658736226.png)

La Qtable est form√© sous forme de dictionnaire r√©cursif de N+1 dimension, N √©tant le nombre total de ligne visible.
L'id√©e est de fusionner ensemble les cl√©s communes afin de ne pas consommer trop de m√©moire vive (on peut avoir plus de
5 millions de cl√©s diff√©rentes). La premi√®re dimension repr√©sente la premi√®re ligne visible, la deuxi√®me dimension la
seconde, ... La derni√®re dimension repr√©sente les actions possibles.

![Qtable](./doc/README-1667659666276.png)

![Qtable](./doc/README-1667659673471.png)

#### Apprentissage

Nous avons test√© diff√©rents hyper-param√®tres pour l'apprentissage sur plusieurs heures.
Les r√©sultats les plus convaincants ont √©t√© obetnus avec les param√®tres suivants :

- `AGENT_GAMMA = 0.1`
- `AGENT_LEARNING_RATE = 0.6`
- `AGENT_VISIBLE_COLS_ARROUND = 4`
- `AGENT_VISIBLE_LINES_ABOVE = 2 # 1`
- `EXPLORE_RATE = 0.1`
- `EXPLORE_RATE_DECAY = 0.9999`

Nous avoisinons un taux de r√©ussite plus haut (~95%) avec une ligne visible devant contre ~90% avec 2 lignes visibles.
Cependant, nous avont remarqu√© que les mouvements de la grenouille √©taient plus fluides avec 2 lignes visibles, car une
meilleure anticipation.

![QLEARNING_L1_C4.history.png](./doc/README-1667736022512.png)

![QLEARNING_L2_C4.history.png](./doc/README-1667736019516.png)

Les pics sont d√ªs au rechargement du taux d'exploration (1x par heure)

### Multi Q-Learning

Lors de l'apprentissage par le Q-Learning, nous avons remarqu√© que nous avions beaucoup d'√©tats diff√©rents, plusieurs
millions. Pourtant, beaucoup d'√©tat sont similaires, ont des lignes identiques, mais sont consi√©d√©r√©s comme des √©tats
diff√©rents.

Dans le cas o√π nous avons 4 lignes visibles, ainsi que 4 colonnes visibles.
Pour 5 tokens diff√©rents, avec une grille de (17x4) 68 cases, nous avons en th√©orie 68^5 = 1,453,933,568 possibilit√©s
max, ce qui est GIGANTESQUE.

Pour r√©soudre ce probl√®me, nous avons impl√©ment√© le Multi Q-Learning. Cette m√©thode consiste √† s√©parer notre unique
Qtable en 3 :

- Une Qtable pour les lignes du haut, ne g√©rant que l'action de monter
- Une Qtable pour les lignes du bas, ne g√©rant que l'action de descendre
- Une Qtable pour la ligne centrale, ne g√©rant que les actions de gauche, droite ou de rester

√Ä chaque it√©ration, nous fusionnons la liste d'action possible en fonction de l'environnement donn√©e.

Ainsi, nous r√©duisons le nombre d'√©tats possible (pour la m√™me configuration) √† (2x17)^5 + 17^5 + 17^5 = 48,275,138
possibilit√©s max, ce qui est beaucoup plus raisonnable.

#### Impl√©mentation

L'impl√©mentation du Multi Q-Learning est g√©r√© par la classe [`MultiQtable`](./ai/MultiQtable.py). Cette derni√®re
permet de g√©rer les 3 Qtables, de mettre √† jour les valeurs de la table, de r√©cup√©rer la meilleure action √† effectuer,
de sauvegarder les tables de Q-Learning, ...

![](./doc/README-1667662135336.png)

![](./doc/README-1667662138576.png)

![](./doc/README-1667662140907.png)

![](./doc/README-1667662142763.png)

Cette classe permet √©galement (comme les autres m√©thodes d'apprentissage) de g√©rer l'exploration et l'exploitation,
ainsi que l'historique de progression.

La classe est sensiblement la m√™me que celle du Q-Learning, √† la diff√©rence que nous avons 3 Qtables, que nous
fusionnons au moment o√π il est n√©c√©ssaire et r√©cup√©rer / mettre √† jour le poids des actions selon un √©tat donn√©.

```python
from typing import Dict

from ai.Model import Model
from conf.config import ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT, ACTION_NONE


class MultiQtable(Model):
  def __init__(self, alpha: float, gamma: float, score_history_packets: int, visible_lines_above: int):
    self.__visible_lines_above = visible_lines_above
    self.__qtable: Dict[str: Dict[str: float]] = {"UP": {}, "CENTER": {}, "DOWN": {}}
    # ...

  def get_state_actions(self, state: [str]) -> Dict[str, float]:
    actions = {}
    up_state = "\n".join(state[:self.__visible_lines_above])
    center_state = "\n".join(state[self.__visible_lines_above:self.__visible_lines_above + 1])
    down_state = "\n".join(state[self.__visible_lines_above + 1:self.__visible_lines_above + 2])

    if up_state not in self.__qtable["UP"]:
      self.__qtable["UP"][up_state] = {ACTION_UP: 0}
    up = self.__qtable["UP"][up_state]

    if center_state not in self.__qtable["CENTER"]:
      self.__qtable["CENTER"][center_state] = {
        ACTION_LEFT: 0, ACTION_RIGHT: 0, ACTION_NONE: 0}
    center = self.__qtable["CENTER"][center_state]

    if down_state not in self.__qtable["DOWN"]:
      self.__qtable["DOWN"][down_state] = {
        ACTION_DOWN: 0}
    down = self.__qtable["DOWN"][down_state]

    actions.update(up)
    actions.update(center)
    actions.update(down)

    return actions

  def update_state(self, state: [str], max_q: float,
                   reward: float,
                   action: str):
    qtable = self.get_state_actions(state)
    new_q = (1 - self.__alpha) * qtable[action] + self.__alpha * (reward + self.__gamma * max_q)

    if action == ACTION_UP:
      up_state = "\n".join(state[:self.__visible_lines_above])
      self.__qtable["UP"][up_state][action] = new_q
    elif action == ACTION_DOWN:
      down_state = "\n".join(state[self.__visible_lines_above + 1:self.__visible_lines_above + 2])
      self.__qtable["DOWN"][down_state][action] = new_q
    else:
      center_state = "\n".join(state[self.__visible_lines_above:self.__visible_lines_above + 1])
      self.__qtable["CENTER"][center_state][action] = new_q

    self.__increment_step_count()
```

#### Apprentissage

Nous avons test√© diff√©rents hyper-param√®tres pour l'apprentissage sur plusieurs heures.
Les r√©sultats les plus convaincants ont √©t√© obetnus avec les param√®tres suivants :

- `AGENT_GAMMA = 0.1`
- `AGENT_LEARNING_RATE = 0.6`
- `AGENT_VISIBLE_COLS_ARROUND = 4 # 6`
- `AGENT_VISIBLE_LINES_ABOVE = 2 # 1`
- `EXPLORE_RATE = 0.1`
- `EXPLORE_RATE_DECAY = 0.9999`

![MQLEARNING_L1_C4.history.png](./doc/README-1667736263139.png)

![MQLEARNING_L1_C6.history.png](./doc/README-1667736265073.png)

![MQLEARNING_L2_C4.history.png](./doc/README-1667736266885.png)

![MQLEARNING_L2_C6.history.png](./doc/README-1667736268641.png)

Les pics sont d√ªs au rechargement du taux d'exploration (1x par heure)

### Deep Q-Learning

Malgr√© tout nos efforts le probl√®me auquel nous faisons face est un probl√®me qui a un grand nombre de possiblit√©. Le
deep Learning permet de s'abstraire de cette contrainte. De faire d√©velopper √† la machine un instinct qui va lui
permettre de choisir une action, qui ne d√©pendera pas du nombre de possibilit√©s.

Encore une fois la class est sensiblement la m√™me que pour la QTable.

Nous avons utilis√© la librairy sklearn dans laquelle nous avons utilis√© MLPRegressor.

Nous avons pris le probl√®me de mani√®re que l'objectif soit de maximiser la meilleure decision. Nous avons donc favoris√©
l'utilisation de regression. Ce qui est coh√©rent avec notre syst√®me de r√©compense.

Nous avons essay√© plusieurs mod√®les :
Des mod√®les avec 1, 2 ou 3 couches cach√©es de neurone.
Nous avons aussi essay√© d'entrainer le mod√®le avec diff√©rente fonction d'activation et diff√©rente solver. (RELU, tanh et
sgd, adam)

Nos r√©sultats les plus probants viennent du mod√®le le plus simple √† entra√Æner qui est le r√©seau √† une couche de 1000
neurones avec la fonction d'activation tanh, Le solver sgd (Stochastic Gradient Descent).

Ayant pass√© beaucoup de temps sur l'apprentissage en Q-Learning, nous avons d√©cid√© de ne pas approfondir le Deep
Q-Learning plus que √ßa.

Version avec une seule couche cach√©e :

```python
self.__mlp = MLPRegressor(
  hidden_layer_sizes=1000,
  activation='tanh',
  solver='sgd',
  learning_rate_init=self.__alpha,
  max_iter=1,
  warm_start=True
)
```

Version avec 3 couches cach√©es :

```python
self.__mlp = MLPRegressor(
  hidden_layer_sizes=(1000, 1000, 1000),
  activation='relu',
  solver='adam',
  learning_rate_init=self.__alpha,
  max_iter=199,
  warm_start=True
)
```

#### Apprentissage avec mod√®le pr√©-entrain√© (Q-Learning)

Nous avons √©galement tent√© de transformer une Q-Table en mod√®le de Deep Q-Learning. Nous esp√©rions que cela permettrait
d'acc√©l√©rer l'apprentissage. Nous avons donc entrain√© un mod√®le avec la Q-Table et nous avons ensuite utilis√© ce mod√®le
pour entra√Æner le mod√®le Deep Q-Learning.

Malheureusement, nous n'avons pas r√©ussi √† obtenir de r√©sultats probants avec cette m√©thode.

## Apprentissage continu

Pour √™tre capable d'apprendre toutes les possibilit√©s, qui peuvent √™tre nombreuses, nous avons besoin de laisser tourner
l'apprentissage pendant plusieurs heures, voire plusieurs jours.

Cela nous ralentit dans la recherche du bon algorithme, des bons param√®tres...
Nous avons donc mis en place tout un tas d'am√©lioration de performances pour pouvoir faire tourner l'apprentissage plus
rapidement. Nous avons √©galement mis en place un syst√®me permettant de faire tourner en mode non graphique (pour plus de
performances) lors de l'apprentissage afin de laisser tourner ce dernier en arri√®re-plan.

### Docker

Nous avons mis en place un environnement docker pour pouvoir faire tourner l'apprentissage dans un conteneur.
Cela nous donne une grande souplesse dans le lancement de nos apprentissages, il suffit juste de changer les avriables
d'environnement.

La difficult√© principale a √©t√© de pouvoir faire tourner le jeu dans un conteneur docker, car il faut faire en sorte que
le jeu se lance en mode `no-graphic` et ne tente pas d'utiliser la librairie `Arcade` afin de ne pas planter...

Nous avons pu ainsi, gr√¢ce √† un serveur et un script `docker-compose`, lancer plusieurs apprentissages en parall√®le sur
notre serveur.

![](./doc/README-1667740782756.png)

```dockerfile
FROM python:3.10

RUN apt-get update \
  && apt-get install -y -qq --no-install-recommends \
    libxext6 \
    libx11-6 \
    libglvnd0 \
    libgl1 \
    libglx0 \
    libegl1 \
    freeglut3-dev \
  && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .
VOLUME /app/save

ENV AGENT_COUNT=5 \
    AGENT_DEBUG=false \
    ARCADE_INSIGHTS=true \
    AGENT_GAMMA=0.1 \
    AGENT_LEARNING_FILE='save/qtable_l2c4.xz' \
    AGENT_LEARNING_RATE=0.6 \
    AGENT_VISIBLE_COLS_ARROUND=4 \
    AGENT_VISIBLE_LINES_ABOVE=2 \
    EXPLORE_RATE=-1 \
    EXPLORE_RATE_DECAY=0.999 \
    GENERATE_HISTORY_GRAPH=true \
    HASH_QTABLE=false \
    LEARNING_MODE=true \
    LEARNING_TYPE=QLEARNING \
    LEARNING_TIME=600 \
    LEARNING_PRINT_STATS_EVERY=60 \
    LEARNING_SAVE_QTABLE_EVERY=300 \
    QTABLE_HISTORY_FILE='save/qtable_l2c4.history' \
    QTABLE_HISTORY_PACKETS=10 \
    WORLD_TYPE=0


CMD [ "python","-u", "./main.py" ]
```

## Probl√®mes rencontr√©s

Pendant la r√©alisation de ce projet, nous avons rencontr√© plusieurs probl√®mes.

### Impl√©mentation du jeu

Le premier challenge √©tait de d√©velopper le jeu, avec une librairie que nous ne connaissions pas, dans un langage que
nous avons peu l'habitude d'utiliser.

Nous sommes partis sur un d√©veloppement en DDD (Domain Driven Design) avec des classes typ√©s, afin de pouvoir facilement
ajouter de nouvelles fonctionnalit√©s (diff√©rents agents d'apprentissage, diff√©rents types de monde, diff√©rents types
d'affichage, ...).

### Apprentissage

Il fallait √©galement mettre au point la fa√ßon dont nous allions faire apprendre notre agent, √† un moment o√π nous
d√©couvrions tout juste le Q-Learning.

Nous avons donc d√ª faire plusieurs tests pour trouver les bons param√®tres, la fa√ßon de repr√©senter notre environnement,
√† l'agent... Au d√©but, nous √©tions fren√© car nous lassions une trop grosse visibilit√© √† l'agent, ce qui faisait que
l'apprentissage ne convergait pas.

Nous avons fini par trouver les r√©glages optimaux apr√®s plusieurs √©ssais.

### Performances

Nous avons √©galement rencontr√© des probl√®mes de performances, notamment lors de l'apprentissage, qui nous ont oblig√© √†
mettre en place des am√©liorations de performances.

Nous avons pass√© plusieurs dizaines d'heures √† optimiser notre code, qui √©tait fonctionnel, pour r√©duire au plus
possible le nombre d'it√©rations, de boucles, etc...

Nous avons √©galement fait passer les librairies python au peigne fin pour trouver les m√©thodes les plus optimis√©s dans
certains cas d'utilisation pour traiter des lots plus rapidement.
Il faut savoir que certaines librairies de python sont plus optimis√©es que d'autres, notamment pour les op√©rations
math√©matiques. Certaines sont √©galements compil√©es en C, ce qui les rend plus performantes.

### Taille des Qtables

Nous avons √©galement rencontr√© des probl√®mes de taille de Qtable, qui nous ont oblig√© √† mettre en place un syst√®me de
compression des Qtables.

Nous avons pu compresser le nombre d'entr√©es gr√¢ce √† un syst√®me d'arbre regrouprant les pr√©fixes communs de notre
Q-table.

Nous avons aussi fait en sorte que la taille de nos caract√®res repr√©sentant nos √©tats soient le plus petit possible,
afin
d'utiliser le moins de caract√®res.

Nous pouvons √©galement activer la hashage de nos √©tats, pour passer d'une ligne de 17 caract√®res (136 bits) √† une ligne
de 8 caract√®res (64 bits). Cela nous permet de garder un peu d'espace en m√©moire et sur le disque.

Lorsque nous sauvegardons nos Qtables, nous les compressons √©galement (LZMA), afin de r√©duire leur taille.

## Conclusion

Au d√©part, nous ne pensions pas que le sujet allait √™tre si complexe pour un jeu dont les r√®gles sont pourtant simples.
Le simple fait de passer d'un environnement statique √† un environnement stochastique, nous a oblig√© √† repenser la fa√ßon
dont l'agent per√ßoit son environnement et apprend de ce dernier.

Nous avons √©galement appris √† utiliser une nouvelle librairie, et √† d√©velopper en python.

Ce projet nous a fait nous investir √† fond, et nous a permis de d√©couvrir de nouvelles choses.

Nous avons pris plaisir sur la partie optimisation √† d√©couvrir comment rendre notre apprentissage encore plus rapide,
afin d'obtenir des r√©sultats satisfaisant encore plus rapidement. Pour ce faire, nous avons d√ª tout de m√™me laisser
tourner l'apprentissage pendant plusieurs jours sur un serveur d√©di√©.
