# Reinforced Frog

Jumper Frog in python with AI reinforcement üê∏

## Status

| Branch | Quality Gate                                                                                                                                                                                                                                                                                              | Bugs                                                                                                                                                                                                                                                                               | Vulnerabilities                                                                                                                                                                                                                                                                                          | Code Smells                                                                                                                                                                                                                                                                                      | Reliability                                                                                                                                                                                                                                                                                                    | Security                                                                                                                                                                                                                                                                                                 | Maintainability                                                                                                                                                                                                                                                                                              | Technical Debt                                                                                                                                                                                                                                                                                      |
|--------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Main   | [![Quality Gate Status](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=alert_status&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Bugs](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=bugs&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Vulnerabilities](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=vulnerabilities&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Code Smells](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=code_smells&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Reliability Rating](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=reliability_rating&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Security Rating](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=security_rating&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Maintainability Rating](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=sqale_rating&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) | [![Technical Debt](https://sonar.nospy.fr/api/project_badges/measure?branch=main&project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=sqale_index&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&branch=main) |
| Dev    | [![Quality Gate Status](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=alert_status&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Bugs](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=bugs&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Vulnerabilities](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=vulnerabilities&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Code Smells](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=code_smells&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Reliability Rating](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=reliability_rating&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Security Rating](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=security_rating&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Maintainability Rating](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=sqale_rating&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         | [![Technical Debt](https://sonar.nospy.fr/api/project_badges/measure?project=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1&metric=sqale_index&token=8fb6fe4f86f2d76bd5ee6280538490bad0cc6ef8)](https://sonar.nospy.fr/dashboard?id=Nouuu_reinforced-frog_AYNkrhosm-3jSFiMinj1)                         |

|                                                             |                                            |
|-------------------------------------------------------------|--------------------------------------------|
| ![README-1666710399073.gif](./doc/README-1666710399073.gif) | <img width="250px" src="armored_frog.png"> |

## Pr√©sentation du jeu et Objectif

L'objectif principal est de faire apprendre par renforcement un agent sur le jeu Frogger.

### Contexte

Ce projet a √©t√© r√©alis√© dans le cadre du cours d'apprentissage par renforcement. Il a √©t√© r√©alis√© par 3 √©tudiants en
5·µâ ann√©e d'architecture logicielle.

### Frogger original

| R√®gles du jeu                                                                                                                                                                                                                                                                                                                                                                         | Jeu original                                            |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|
| Frogger est un jeu d'arcade classique. Le but du jeu est de diriger des grenouilles jusqu'√† leurs maisons. Pour cela, le joueur doit d'abord traverser une route en √©vitant des voitures qui roulent √† diff√©rentes vitesses puis une rivi√®re aux courants changeants et enfin, √† nouveaux une route. La grenouille meurt si elle touche une voiture ou si elle tombe dans la rivi√®re. | <img width="1500px" src="doc/README-1666882432331.png"> |

### Objectif

L'objectif est de faire apprendre √† un agent √† traverser la route et la rivi√®re en √©vitant les voitures et l'eau.
Pour cela, nuos allons utiliser l'algorithme `Q-Learning`. L'agent va apprendre √† traverser la route et la rivi√®re en
apprenant √† associer une action √† un √©tat. L'agent va donc apprendre √† associer une action √† un √©tat.

Pour cela, nous allons √©galement devoir d√©velopper le jeu Frogger en utilisant la
librairie [`arcade`](https://api.arcade.academy/en/latest/). Le seul langage
utilis√© est le Python, nous n'utilisons pas de librairie externe mis √† part arcade et quelques librairies utilitaires.

## Installation

### Pr√©requis

## Utilisation

### Environnement

## D√©veloppement du jeu

### Pr√©sentation de la librairie arcade

[`Arcade`](https://api.arcade.academy/en/latest/) est une librairie Python permettant de cr√©er des jeux vid√©o. Elle est
bas√©e sur Pyglet et permet de cr√©er des jeux vid√©o 2D. Elle permet de cr√©er des jeux vid√©o en 2D avec des sprites, des
animations, des sons, des effets de particules, ...

![img.png](doc/img.png)

### Configuration des r√®gles

Afin de pouvoir modifier rapidement la configuration de notre jeu (difficult√©, tokens, actions possibles, ...), nous
avons
√©crit toute la configuration dans le fichier [`config.py`](./conf/config.py). Ce fichier est lu par le jeu.

#### Tokens

Ce fichier de configuration contient les diff√©rents tokens utilis√©s dans le jeu. Ces derniers permettent au jeu d'avoir
une repr√©sentation textuelle de son environement, ce qui va grandement nous aider pour l'apprentissage de l'agent.

```python
CAR_TOKEN = 'C'
TRUCK_TOKEN = 'Z'
TURTLE_TOKEN = 'T'
TURTLE_L_TOKEN = 'TL'
TURTLE_XL_TOKEN = 'TXL'
REVERSED_CAR_TOKEN = 'RC'
REVERSED_TRUCK_TOKEN = 'RZ'
REVERSED_TURTLE_TOKEN = 'RT'
REVERSED_TURTLE_L_TOKEN = 'RTL'
...

ACTION_UP = 'U'
ACTION_DOWN = 'D'
ACTION_LEFT = 'L'
ACTION_RIGHT = 'R'
ACTION_NONE = 'N'
...

WATER_COMMONS_TOKENS = [TURTLE_TOKEN, TURTLE_L_TOKEN, TURTLE_XL_TOKEN, REVERSED_TURTLE_TOKEN, REVERSED_TURTLE_L_TOKEN,
                        ...]
...

WIN_STATES = [EXIT_TOKEN]

```

#### Arcade

Ce fichier de configuration contient les diff√©rents param√®tres de la librairie arcade. Ces derniers permettent de
d√©finir les sprites des diff√©rentes entit√©s, ainsi que leur taille et le scaling.

```python
SCALE = 1
SPRITE_SIZE = 64 * SCALE

...


def get_sprite_resources(name: str, sprite_size: float = 0.5):
  return arcade.Sprite(f":resources:images/{name}.png", sprite_size * SCALE)


def get_sprite_local(name: str, sprite_size: float = 0.5):
  return arcade.Sprite(f"assets/sprite/{name}.png", sprite_size * SCALE)


ENTITIES: Dict[str, WorldEntity] = {
  CAR_TOKEN: WorldEntity(1, 1, CAR_TOKEN, get_sprite_local("car_1", 0.65)),
  ...
}

...

WORLD_WIDTH = 180
WORLD_HEIGHT = 117
WORLD_SCALING = 9
```

### Repr√©sentation du monde

Le monde est repr√©sent√© par une matrice de caract√®res. Chaque caract√®re repr√©sente une entit√© du monde. Les entit√©s
sont repr√©sent√©es par des tokens. Ces derniers sont d√©finis dans le fichier de configuration.

La classe permettant de repr√©senter le monde est la classe [`World`](./game/world.py). Cette classe permet de

Cette derni√®re permet de :

- Cr√©er un monde, avec la bonne configuration
- G√©rer la mise √† jour (d√©placement) des entit√©s dans le monde
- G√©rer les collisions entre les entit√©s
- G√©rer les mouvements, r√©compenses des joueurs

#### Repr√©sentation du monde pour l'agent

A chaque √©tat, l'agent re√ßoit une repr√©sentation du monde sous forme de liste de cha√Æne caract√®res. Chaque √©l√©ment
repr√©sente une ligne visible ( d√©finit dans les variables d'environnement) du monde.

Ainsi, l'agent ne voit pas toute la carte, mais tout au plus 2 ligne devant lui, 1 ligne derri√®re lui, et 4 colonnes sur
les c√¥t√©s.

![Untitled Diagram.png](./doc/README-1667658404493.png)

#### World Entity

Pour g√©n√©raliser les diff√©rentes entit√©s que nous traitons, nous avons la
classe [`WorldEntity`](./display/entity/world_entity.py).
Cette derni√®re permet de regrouper pour chaques entit√©s :

- La taille de l'entit√©
- Le token de l'entit√©
- Le sprite de l'entit√©

#### World Line

La classe [`WorldLine`](./display/entity/world_line.py) permet de repr√©senter une ligne du monde. Cette derni√®re permet
de g√©rer les d√©placements des entit√©s sur chaque ligne, ainsi que leur fr√©quence d'apparition, vitesse, ...

### Joueurs

Le joueur est repr√©sent√© par l'interface [`Player`](./game/Player.py). Cette derni√®re permet de g√©rer le d√©placement du
joueur, de le g√©rer dans la classe principale `Game`. Cette interface nous permet de g√©rer plusieurs types de joueurs (
Humain, Agent).

```python
from typing import Tuple, List

from arcade import Sprite

from display.entity.world_entity import WorldEntity
from game.world import World


class Player:
  def init(self, world: World, intial_state: Tuple[int, int], _initial_environment: bytes):
    pass

  def best_move(self, environment: [str]) -> str:
    pass

  def step(self, action: str, reward: float, new_state: Tuple[int, int], _environment: List[str]):
    pass

  def save_score(self):
    pass

  def update_state(self, new_state, new_environment):
    pass

  @property
  def sprite(self) -> Sprite:
    pass

  @property
  def world_entity(self) -> WorldEntity:
    pass

  @property
  def is_human(self) -> bool:
    pass

  @property
  def score(self) -> int:
    pass

  @property
  def state(self) -> Tuple[int, int]:
    pass
```

#### Joueur Humain

Le joueur humain est repr√©sent√© par la classe [`HumanPlayer`](./game/HumanPlayer.py). Cette derni√®re permet de se
d√©placer avec les touches directionnelles du clavier.

### Affichage graphique

L'affichage graphique est enti√®rement g√©r√© par la classe [`WorldWindow`](./display/world_window.py). Cette derni√®re
permet de g√©rer l'affichage du monde sur arcade. Elle permet √©galement de g√©rer la vitesse de jeu, le nombre de frames
par seconde, les statistiques affich√©s, ...

```python
import arcade.color

from ai.Model import Model
from conf.config import *
from game.game import Game


class WorldWindow(arcade.Window):
  def __init__(self, game: Game, env, model: Model):
    super().__init__(
      int(game.world.width / WORLD_SCALING * SPRITE_SIZE),
      int(game.world.height / WORLD_SCALING * SPRITE_SIZE),
      'REINFORCED FROG',
      update_rate=1 / 60
    )
    self.__height = int(game.world.height / WORLD_SCALING * SPRITE_SIZE)

  # ...

  # ...

  def setup(self):
    self.setup_world_states()
    self.setup_players_states()
    self.setup_world_entities_state()

  def setup_world_entities_state(self):
    self.__entities_sprites = arcade.SpriteList()
    for state in self.__game.world.world_entities_states.keys():
      world_entity: WorldEntity = self.__game.world.get_world_entity(state)
      if world_entity is not None:
        sprite = self.__get_entity_sprite(state, world_entity)
        self.__entities_sprites.append(sprite)

  def setup_world_states(self):
    self.__world_sprites = arcade.SpriteList()
    for state in self.__game.world.world_states:
      world_entity: WorldEntity = self.__game.world.get_world_line_entity(state)
      if world_entity is not None:
        sprite = self.__get_environment_sprite(state, world_entity)
        self.__world_sprites.append(sprite)

  def setup_players_states(self):
    self.__players_sprites = arcade.SpriteList()
    for player in self.__game.players:
      sprite = player.sprite
      sprite.center_x, sprite.center_y = (
        self.__get_xy_state((player.state[0] + WORLD_SCALING // 2, player.state[1] + WORLD_SCALING // 2)))
      self.__players_sprites.append(sprite)

  def __draw_debug(self):

  # ...

  def on_draw(self):
    arcade.start_render()
    self.__world_sprites.draw()
    self.__entities_sprites.draw()
    self.__players_sprites.draw()
    if self.__debug == 1:
      self.__draw_debug()
    elif self.__debug == 2:
      self.__draw_collisions_debug()
    if self.__env['ARCADE_INSIGHTS']:
      self.__draw_model_insights()

  def __draw_model_insights(self):

  # ...

  def on_update(self, delta_time: float):
    self.__game.step()
    self.setup_players_states()
    self.__players_sprites.update()
    self.setup_world_entities_state()
    self.__entities_sprites.update()

  # ...
```

## D√©veloppement de l'IA

Pour l'IA, nous avons utilis√© 3 m√©thodes principales :

- Q-Learning
- Deep Q-Learning
- Multi-Q-Learning

### Q-Learning

Le Q-Learning est une m√©thode d'apprentissage par renforcement. Cette m√©thode permet de d√©terminer la meilleure action
√† effectuer dans un √©tat donn√©. Pour cela, elle utilise une fonction de valeur Q qui permet de d√©terminer la valeur
d'une action dans un √©tat donn√©. Cette fonction est mise √† jour √† chaque √©tape de l'apprentissage.

#### Impl√©mentation

L'impl√©mentation du Q-Learning est g√©r√© par la classe [`QLearning`](./ai/Qtable.py). Cette derni√®re permet de
g√©rer la table de Q-Learning, de mettre √† jour les valeurs de la table, de r√©cup√©rer la meilleure action √† effectuer,
de sauvegarder la table de Q-Learning, ...

Cette classe permet √©galement (comme les autres m√©thodes d'apprentissage) de g√©rer l'exploration et l'exploitation,
ainsi que l'historique de progression.

Nous nous basons sur l'√©quation de Bellman pour mettre √† jour la table de Q-Learning :

![0 EezURPNjW2U6EYpA.jpeg](./doc/README-1667658736226.png)

La Qtable est form√© sous forme de dictionnaire r√©cursif de N+1 dimension, N √©tant le nombre total de ligne visible.
L'id√©e est de fusionner ensemble les cl√©s communes afin de ne pas consommer trop de m√©moire vive (on peut avoir plus de
5 millions de cl√©s diff√©rentes). La premi√®re dimension repr√©sente la premi√®re ligne visible, la deuxi√®me dimension la
seconde, ... La derni√®re dimension repr√©sente les actions possibles.

![Qtable](./doc/README-1667659666276.png)

![Qtable](./doc/README-1667659673471.png)

#### Apprentissage

Nous avons test√© diff√©rents hyper-param√®tres pour l'apprentissage sur plusieurs heures.
Les r√©sultats les plus convaincants ont √©t√© obetnus avec les param√®tres suivants :

- `AGENT_GAMMA = 0.1`
- `AGENT_LEARNING_RATE = 0.6`
- `AGENT_VISIBLE_COLS_ARROUND = 4`
- `AGENT_VISIBLE_LINES_ABOVE = 2 # 1`
- `EXPLORE_RATE = 0.1`
- `EXPLORE_RATE_DECAY = 0.9999`

Nous avoisinons un taux de r√©ussite plus haut (~95%) avec une ligne visible devant contre ~90% avec 2 lignes visibles.
Cependant, nous avont remarqu√© que les mouvements de la grenouille √©taient plus fluides avec 2 lignes visibles, car une
meilleure anticipation.

![QLEARNING_L1_C4.history.png](./doc/README-1667736022512.png)

![QLEARNING_L2_C4.history.png](./doc/README-1667736019516.png)

Les pics sont d√ªs au rechargement du taux d'exploration (1x par heure)

### Multi Q-Learning

Lors de l'apprentissage par le Q-Learning, nous avons remarqu√© que nous avions beaucoup d'√©tats diff√©rents, plusieurs
millions. Pourtant, beaucoup d'√©tat sont similaires, ont des lignes identiques, mais sont consi√©d√©r√©s comme des √©tats
diff√©rents.

Dans le cas o√π nous avons 4 lignes visibles, ainsi que 4 colonnes visibles.
Pour 5 tokens diff√©rents, avec une grille de (17x4) 68 cases, nous avons en th√©orie 68^5 = 1,453,933,568 possibilit√©s
max, ce qui est GIGANTESQUE.

Pour r√©soudre ce probl√®me, nous avons impl√©ment√© le Multi Q-Learning. Cette m√©thode consiste √† s√©parer notre unique
Qtable en 3 :

- Une Qtable pour les lignes du haut, ne g√©rant que l'action de monter
- Une Qtable pour les lignes du bas, ne g√©rant que l'action de descendre
- Une Qtable pour la ligne centrale, ne g√©rant que les actions de gauche, droite ou de rester

√Ä chaque it√©ration, nous fusionnons la liste d'action possible en fonction de l'environnement donn√©e.

Ainsi, nous r√©duisons le nombre d'√©tats possible (pour la m√™me configuration) √† (2x17)^5 + 17^5 + 17^5 = 48,275,138
possibilit√©s max, ce qui est beaucoup plus raisonnable.

#### Impl√©mentation

L'impl√©mentation du Multi Q-Learning est g√©r√© par la classe [`MultiQtable`](./ai/MultiQtable.py). Cette derni√®re
permet de g√©rer les 3 Qtables, de mettre √† jour les valeurs de la table, de r√©cup√©rer la meilleure action √† effectuer,
de sauvegarder les tables de Q-Learning, ...

![](./doc/README-1667662135336.png)

![](./doc/README-1667662138576.png)

![](./doc/README-1667662140907.png)

![](./doc/README-1667662142763.png)

Cette classe permet √©galement (comme les autres m√©thodes d'apprentissage) de g√©rer l'exploration et l'exploitation,
ainsi que l'historique de progression.

La classe est sensiblement la m√™me que celle du Q-Learning, √† la diff√©rence que nous avons 3 Qtables, que nous
fusionnons au moment o√π il est n√©c√©ssaire et r√©cup√©rer / mettre √† jour le poids des actions selon un √©tat donn√©.

```python
from typing import Dict

from ai.Model import Model
from conf.config import ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT, ACTION_NONE


class MultiQtable(Model):
  def __init__(self, alpha: float, gamma: float, score_history_packets: int, visible_lines_above: int):
    self.__visible_lines_above = visible_lines_above
    self.__qtable: Dict[str: Dict[str: float]] = {"UP": {}, "CENTER": {}, "DOWN": {}}
    # ...

  def get_state_actions(self, state: [str]) -> Dict[str, float]:
    actions = {}
    up_state = "\n".join(state[:self.__visible_lines_above])
    center_state = "\n".join(state[self.__visible_lines_above:self.__visible_lines_above + 1])
    down_state = "\n".join(state[self.__visible_lines_above + 1:self.__visible_lines_above + 2])

    if up_state not in self.__qtable["UP"]:
      self.__qtable["UP"][up_state] = {ACTION_UP: 0}
    up = self.__qtable["UP"][up_state]

    if center_state not in self.__qtable["CENTER"]:
      self.__qtable["CENTER"][center_state] = {
        ACTION_LEFT: 0, ACTION_RIGHT: 0, ACTION_NONE: 0}
    center = self.__qtable["CENTER"][center_state]

    if down_state not in self.__qtable["DOWN"]:
      self.__qtable["DOWN"][down_state] = {
        ACTION_DOWN: 0}
    down = self.__qtable["DOWN"][down_state]

    actions.update(up)
    actions.update(center)
    actions.update(down)

    return actions

  def update_state(self, state: [str], max_q: float,
                   reward: float,
                   action: str):
    qtable = self.get_state_actions(state)
    new_q = (1 - self.__alpha) * qtable[action] + self.__alpha * (reward + self.__gamma * max_q)

    if action == ACTION_UP:
      up_state = "\n".join(state[:self.__visible_lines_above])
      self.__qtable["UP"][up_state][action] = new_q
    elif action == ACTION_DOWN:
      down_state = "\n".join(state[self.__visible_lines_above + 1:self.__visible_lines_above + 2])
      self.__qtable["DOWN"][down_state][action] = new_q
    else:
      center_state = "\n".join(state[self.__visible_lines_above:self.__visible_lines_above + 1])
      self.__qtable["CENTER"][center_state][action] = new_q

    self.__increment_step_count()
```

#### Apprentissage

Nous avons test√© diff√©rents hyper-param√®tres pour l'apprentissage sur plusieurs heures.
Les r√©sultats les plus convaincants ont √©t√© obetnus avec les param√®tres suivants :

- `AGENT_GAMMA = 0.1`
- `AGENT_LEARNING_RATE = 0.6`
- `AGENT_VISIBLE_COLS_ARROUND = 4 # 6`
- `AGENT_VISIBLE_LINES_ABOVE = 2 # 1`
- `EXPLORE_RATE = 0.1`
- `EXPLORE_RATE_DECAY = 0.9999`

![MQLEARNING_L1_C4.history.png](./doc/README-1667736263139.png)

![MQLEARNING_L1_C6.history.png](./doc/README-1667736265073.png)

![MQLEARNING_L2_C4.history.png](./doc/README-1667736266885.png)

![MQLEARNING_L2_C6.history.png](./doc/README-1667736268641.png)

Les pics sont d√ªs au rechargement du taux d'exploration (1x par heure)

### Deep Q-Learning

#### Impl√©mentation

#### Apprentissage

#### Apprentissage avec mod√®le pr√©-entrain√© (Q-Learning)

## Apprentissage continu

Pour √™tre capable d'apprendre toutes les possibilit√©s, qui peuvent √™tre nombreuses, nous avons besoin de laisser tourner
l'apprentissage pendant plusieurs heures, voire plusieurs jours.

Cela nous ralentit dans la recherche du bon algorithme, des bons param√®tres...
Nous avons donc mis en place tout un tas d'am√©lioration de performances pour pouvoir faire tourner l'apprentissage plus
rapidement. Nous avons √©galement mis en place un syst√®me permettant de faire tourner en mode non graphique (pour plus de
performances) lors de l'apprentissage afin de laisser tourner ce dernier en arri√®re-plan.

### Docker

Nous avons mis en place un environnement docker pour pouvoir faire tourner l'apprentissage dans un container.
Cela nous donne une grande souplesse dans le lancement de nos apprentissages, il suffit juste de changer les avriables
d'environnement.

## Probl√®mes rencontr√©s

### Impl√©mentation du jeu

### Apprentissage

### Performances

### Taille des Qtables

## Conclusion
